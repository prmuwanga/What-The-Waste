{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import load_img\n",
    "from sklearn.metrics import classification_report \n",
    "import numpy as np\n",
    "import sklearn\n",
    "import os \n",
    "import shutil \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18042 images belonging to 13 classes.\n",
      "{'battery': 0, 'biological': 1, 'brown-glass': 2, 'cardboard': 3, 'clothes': 4, 'glass': 5, 'green-glass': 6, 'metal': 7, 'paper': 8, 'plastic': 9, 'shoes': 10, 'trash': 11, 'white-glass': 12}\n"
     ]
    }
   ],
   "source": [
    "train_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Turning every folder name into class label\n",
    "train_data = train_gen.flow_from_directory(\n",
    "    \"../Datasets/combined-cleaned-dataset\",\n",
    "    target_size=(224,224),\n",
    "    batch_size=32,\n",
    "    class_mode= \"categorical\"\n",
    ")\n",
    "\n",
    "print(train_data.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " battery: 661 train / 141 val / 143 test\n",
      " biological: 689 train / 147 val / 149 test\n",
      " brown-glass: 424 train / 91 val / 92 test\n",
      " cardboard: 905 train / 194 val / 195 test\n",
      " clothes: 3727 train / 798 val / 800 test\n",
      " glass: 350 train / 75 val / 76 test\n",
      " green-glass: 440 train / 94 val / 95 test\n",
      " metal: 825 train / 176 val / 178 test\n",
      " paper: 1150 train / 246 val / 248 test\n",
      " plastic: 942 train / 202 val / 203 test\n",
      " shoes: 1383 train / 296 val / 298 test\n",
      " trash: 583 train / 125 val / 126 test\n",
      " white-glass: 542 train / 116 val / 117 test\n"
     ]
    }
   ],
   "source": [
    "# Spliting our dataset into training, validation, and testing \n",
    "def split_dataset(source_dir, output_dir, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
    "    random.seed(42)\n",
    "    \n",
    "    for class_folder in os.listdir(source_dir):\n",
    "        class_path = os.path.join(source_dir, class_folder)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue \n",
    "        \n",
    "        images = [f for f in os.listdir(class_path) if f.lower().endswith(('.jpg','.jpeg','.png'))]\n",
    "        random.shuffle(images)\n",
    "    \n",
    "        total = len(images)\n",
    "        train_end = int(total * train_ratio)\n",
    "        val_end = train_end + int(total * val_ratio)\n",
    "    \n",
    "        train_images = images[:train_end]\n",
    "        val_images = images[train_end:val_end]\n",
    "        test_images = images[val_end:]\n",
    "    \n",
    "        train_class_dir = os.path.join(output_dir, 'train', class_folder)\n",
    "        val_class_dir = os.path.join(output_dir, 'val', class_folder)\n",
    "        test_class_dir = os.path.join(output_dir,'test',class_folder)\n",
    "    \n",
    "        os.makedirs(train_class_dir, exist_ok=True)\n",
    "        os.makedirs(val_class_dir, exist_ok=True)\n",
    "        os.makedirs(test_class_dir, exist_ok=True)\n",
    "    \n",
    "        for img in train_images:\n",
    "            shutil.copy2(os.path.join(class_path, img), os.path.join(train_class_dir, img))\n",
    "\n",
    "        for img in val_images:\n",
    "            shutil.copy2(os.path.join(class_path, img), os.path.join(val_class_dir, img))\n",
    "\n",
    "        for img in test_images:\n",
    "            shutil.copy2(os.path.join(class_path, img), os.path.join(test_class_dir, img))\n",
    "\n",
    "        print(f\" {class_folder}: {len(train_images)} train / {len(val_images)} val / {len(test_images)} test\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    source = \"../Datasets/combined-cleaned-dataset\"   \n",
    "    destination = \"../Notebooks/the_final_sortdown\"          \n",
    "    split_dataset(source, destination, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "Found 16410 files belonging to 13 classes.\n",
      "\n",
      "Validation Data:\n",
      "Found 4985 files belonging to 13 classes.\n",
      "\n",
      "Testing Data:\n",
      "Found 5047 files belonging to 13 classes.\n"
     ]
    }
   ],
   "source": [
    "# Data Augmentation\n",
    "train_gen = ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    rotation_range = 20,\n",
    "    width_shift_range = 0.1,\n",
    "    height_shift_range = 0.1,\n",
    "    brightness_range = [0.8, 1.2]\n",
    ")\n",
    "\n",
    "print(\"Training Data:\")\n",
    "train_data = tf.keras.utils.image_dataset_from_directory(\n",
    "    \"the_final_sortdown/train\",\n",
    "    label_mode='categorical',\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")\n",
    "class_names = train_data.class_names\n",
    "\n",
    "\n",
    "val_gen = ImageDataGenerator(rescale=1./255)\n",
    "test_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "print(\"\\nValidation Data:\")\n",
    "val_data = tf.keras.utils.image_dataset_from_directory(\n",
    "    \"the_final_sortdown/val\",\n",
    "    label_mode='categorical',\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")\n",
    "print(\"\\nTesting Data:\")\n",
    "test_data = tf.keras.utils.image_dataset_from_directory(\n",
    "    \"the_final_sortdown/test\",\n",
    "    label_mode='categorical',\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer Learning MobileNetV2\n",
    "base_model = MobileNetV2(\n",
    "    include_top = False,\n",
    "    weights ='imagenet',\n",
    "    input_shape = (224,224,3)\n",
    ")\n",
    "base_model.trainable = False\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "\n",
    "num_classes = len(train_data.class_names)\n",
    "output = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(),\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',   # What to monitor (validation loss is typical)\n",
    "    patience=5,           # How many epochs without improvement before stopping\n",
    "    restore_best_weights=True  # Optional, but restores the best model, not the last one\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "\u001b[1m513/513\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m291s\u001b[0m 562ms/step - accuracy: 0.5695 - loss: 1.3950 - val_accuracy: 0.7135 - val_loss: 0.9101\n",
      "Epoch 2/7\n",
      "\u001b[1m513/513\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 511ms/step - accuracy: 0.7182 - loss: 0.8830 - val_accuracy: 0.7503 - val_loss: 0.7894\n",
      "Epoch 3/7\n",
      "\u001b[1m513/513\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m281s\u001b[0m 548ms/step - accuracy: 0.7576 - loss: 0.7515 - val_accuracy: 0.7775 - val_loss: 0.6862\n",
      "Epoch 4/7\n",
      "\u001b[1m513/513\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m279s\u001b[0m 545ms/step - accuracy: 0.7869 - loss: 0.6656 - val_accuracy: 0.7559 - val_loss: 0.7892\n",
      "Epoch 5/7\n",
      "\u001b[1m513/513\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m271s\u001b[0m 528ms/step - accuracy: 0.8003 - loss: 0.6175 - val_accuracy: 0.8078 - val_loss: 0.6027\n",
      "Epoch 6/7\n",
      "\u001b[1m513/513\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m277s\u001b[0m 539ms/step - accuracy: 0.8150 - loss: 0.5538 - val_accuracy: 0.7922 - val_loss: 0.6237\n",
      "Epoch 7/7\n",
      "\u001b[1m513/513\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m276s\u001b[0m 538ms/step - accuracy: 0.8337 - loss: 0.5010 - val_accuracy: 0.8098 - val_loss: 0.5878\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_data,\n",
    "    validation_data=val_data,\n",
    "    epochs = 7, \n",
    "    callbacks = [early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 390ms/step - accuracy: 0.0322 - loss: 2.9960\n",
      "Test accuarcy: 0.06\n"
     ]
    }
   ],
   "source": [
    "-\n",
    "test_loss, test_acc = model.evaluate(test_data)\n",
    "print(f\"Test accuarcy: {test_acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 407ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     battery       0.00      0.00      0.00       265\n",
      "  biological       0.10      0.03      0.05       268\n",
      " brown-glass       0.02      0.04      0.02       173\n",
      "   cardboard       0.00      0.00      0.00       360\n",
      "     clothes       0.33      0.00      0.00      1493\n",
      "       glass       0.00      0.00      0.00       140\n",
      " green-glass       0.00      0.00      0.00       175\n",
      "       metal       0.06      0.68      0.11       333\n",
      "       paper       0.19      0.03      0.04       470\n",
      "     plastic       0.00      0.00      0.00       381\n",
      "       shoes       0.00      0.00      0.00       545\n",
      "       trash       0.05      0.07      0.06       235\n",
      " white-glass       0.05      0.02      0.03       209\n",
      "\n",
      "    accuracy                           0.06      5047\n",
      "   macro avg       0.06      0.07      0.02      5047\n",
      "weighted avg       0.13      0.06      0.02      5047\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prmuw\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\prmuw\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\prmuw\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Adding Precision, Recall, and F1\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(test_data)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Get true labels\n",
    "y_true = []\n",
    "for images, labels in test_data.unbatch():\n",
    "    y_true.append(np.argmax(labels.numpy()))\n",
    "y_true = np.array(y_true)\n",
    "\n",
    "# Print report\n",
    "print(classification_report(y_true, y_pred_classes, target_names=class_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Reinforcement Learning Sysytem\n",
    "\n",
    "import gym \n",
    "from gym import spaces \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WasteDisposalEnv(gym.Env): \n",
    "\n",
    "    def __init__(self, classifier_model, test_data, co2_impact_mapping):\n",
    "\n",
    "        super(WasteDisposalEnv, self).__init__()\n",
    "\n",
    "        #used for predicting material type \n",
    "\n",
    "        self.classfier = classifier_model \n",
    "\n",
    "        #data for predict \n",
    "\n",
    "        self.data = test_data\n",
    "        self.images, self.labels = self.get_data()  \n",
    "        self.co2_impact_mapping = co2_impact_mapping \n",
    "\n",
    "        #define action space 4 disposal options(Recycle, Compost, Donate, Landfill)\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(self.classifier.output_shape[-1],), dtype=np.float32)\n",
    "        \n",
    "        self.current_idx = 0\n",
    "\n",
    "\n",
    "    def get_data(self): \n",
    "        images = []\n",
    "        labels = []\n",
    "        for batch_x, batch_y in self.data.unbatch():\n",
    "            images.append(batch_x.numpy())\n",
    "            labels.append(np.argmax(batch_y.numpy()))\n",
    "        images = np.array(images)\n",
    "        labels = np.array(labels)\n",
    "        return images, labels\n",
    "        \n",
    "    def reset(self): \n",
    "        self.current_idx = 0 \n",
    "        image = self.images[self.current_idx]\n",
    "        probs = self.classifier.predict(image[np.newaxis, ...])[0]\n",
    "        return probs \n",
    "    \n",
    "    def step(self, action):\n",
    "        image = self.images[self.current_idx]\n",
    "        true_label = self.labels[self.current_idx]\n",
    "\n",
    "        probs = self.classifier.predict(image[np.newaxis, ...])[0]\n",
    "        predicted_class = np.argmax(probs)\n",
    "\n",
    "        reward = self._calculate_reward(predicted_class, action)\n",
    "\n",
    "        self.current_idx += 1\n",
    "        done = self.current_idx >= len(self.images)\n",
    "\n",
    "        if not done:\n",
    "            next_image = self.images[self.current_idx]\n",
    "            next_probs = self.classifier.predict(next_image[np.newaxis, ...])[0]\n",
    "        else:\n",
    "            next_probs = np.zeros_like(probs)\n",
    "\n",
    "        return next_probs, reward, done, {}\n",
    "        \n",
    "    def _calculate_reward(self, predicted_class, action):\n",
    "        item_type = list(self.data.class_indices.keys())[predicted_class]\n",
    "        impact_info = self.co2_impact_mapping.get(item_type, None)\n",
    "\n",
    "        if impact_info is None:\n",
    "            return -1  # Unknown item, small penalty\n",
    "\n",
    "        max_co2_saved = impact_info[\"max_co2_saved\"]\n",
    "        actions = impact_info[\"actions\"]\n",
    "\n",
    "        action_name = {0: \"recycle\", 1: \"compost\", 2: \"donate\", 3: \"landfill\"}.get(action, None)\n",
    "\n",
    "        if action_name is None:\n",
    "            return -1  # Invalid action penalty\n",
    "\n",
    "        # Get multiplier for action\n",
    "        action_multiplier = actions.get(action_name, 0.0)  # 0 if action not listed\n",
    "\n",
    "        # Reward is proportional\n",
    "        reward = max_co2_saved * action_multiplier\n",
    "        return reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "co2_impact_mapping = {\n",
    "    \"plastic\": {\n",
    "        \"max_co2_saved\": 10,  # in kg CO2\n",
    "        \"actions\": {\n",
    "            \"recycle\": 1.0,    # 100% of CO₂ savings if recycled\n",
    "            \"landfill\": 0.0    # 0% savings if landfilled\n",
    "        }\n",
    "    },\n",
    "    \"food_waste\": {\n",
    "        \"max_co2_saved\": 8,\n",
    "        \"actions\": {\n",
    "            \"compost\": 1.0,\n",
    "            \"landfill\": 0.1    # 10% saving (maybe still some methane capture)\n",
    "        }\n",
    "    },\n",
    "    \"electronics\": {\n",
    "        \"max_co2_saved\": 15,\n",
    "        \"actions\": {\n",
    "            \"donate\": 1.0,\n",
    "            \"landfill\": 0.0\n",
    "        }\n",
    "    },\n",
    "    \"textile\": {\n",
    "        \"max_co2_saved\": 12,\n",
    "        \"actions\": {\n",
    "            \"donate\": 1.0,\n",
    "            \"landfill\": 0.2    # Landfilling might still recover some energy\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m env \u001b[38;5;241m=\u001b[39m WasteDisposalEnv(classifier_model\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m, test_data\u001b[38;5;241m=\u001b[39mtest_data, co2_impact_mapping\u001b[38;5;241m=\u001b[39mco2_impact_mapping)\n\u001b[0;32m      4\u001b[0m episode_rewards \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      5\u001b[0m co2_savings \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "env = WasteDisposalEnv(classifier_model=model, test_data=test_data, co2_impact_mapping=co2_impact_mapping)\n",
    "\n",
    "\n",
    "episode_rewards = []\n",
    "co2_savings = []\n",
    "\n",
    "num_episodes = 10  # you can change this to 100 later if you want\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    total_co2_saved = 0\n",
    "     \n",
    "while not done:\n",
    "        # Random action for now (you can replace with a policy later)\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        next_observation, reward, done, info = env.step(action)\n",
    "\n",
    "        total_reward += reward\n",
    "        total_co2_saved += reward  # because reward is proportional to CO₂ saved\n",
    "\n",
    "        observation = next_observation\n",
    "\n",
    "episode_rewards.append(total_reward)\n",
    "co2_savings.append(total_co2_saved)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Episode {episode}: Total Reward = {total_reward}, Total CO₂ Saved = {total_co2_saved:.2f} kg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot total reward (CO₂ saved) per episode\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(episode_rewards, marker='o')\n",
    "plt.title('Total CO₂ Saved per Episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total CO₂ Saved (kg)')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
