{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import load_img\n",
    "from sklearn.metrics import classification_report \n",
    "import numpy as np\n",
    "import sklearn\n",
    "import os \n",
    "import shutil \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18042 images belonging to 13 classes.\n",
      "{'battery': 0, 'biological': 1, 'brown-glass': 2, 'cardboard': 3, 'clothes': 4, 'glass': 5, 'green-glass': 6, 'metal': 7, 'paper': 8, 'plastic': 9, 'shoes': 10, 'trash': 11, 'white-glass': 12}\n"
     ]
    }
   ],
   "source": [
    "train_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Turning every folder name into class label\n",
    "train_data = train_gen.flow_from_directory(\n",
    "    \"../Datasets/combined-cleaned-dataset\",\n",
    "    target_size=(224,224),\n",
    "    batch_size=32,\n",
    "    class_mode= \"categorical\"\n",
    ")\n",
    "\n",
    "print(train_data.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " paper: 1150 train / 246 val / 248 test\n",
      " green-glass: 440 train / 94 val / 95 test\n",
      " clothes: 3727 train / 798 val / 800 test\n",
      " metal: 825 train / 176 val / 178 test\n",
      " cardboard: 905 train / 194 val / 195 test\n",
      " trash: 583 train / 125 val / 126 test\n",
      " glass: 350 train / 75 val / 76 test\n",
      " biological: 689 train / 147 val / 149 test\n",
      " white-glass: 542 train / 116 val / 117 test\n",
      " battery: 661 train / 141 val / 143 test\n",
      " brown-glass: 424 train / 91 val / 92 test\n",
      " plastic: 942 train / 202 val / 203 test\n",
      " shoes: 1383 train / 296 val / 298 test\n"
     ]
    }
   ],
   "source": [
    "# Spliting our dataset into training, validation, and testing \n",
    "def split_dataset(source_dir, output_dir, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
    "    random.seed(42)\n",
    "    \n",
    "    for class_folder in os.listdir(source_dir):\n",
    "        class_path = os.path.join(source_dir, class_folder)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue \n",
    "        \n",
    "        images = [f for f in os.listdir(class_path) if f.lower().endswith(('.jpg','.jpeg','.png'))]\n",
    "        random.shuffle(images)\n",
    "    \n",
    "        total = len(images)\n",
    "        train_end = int(total * train_ratio)\n",
    "        val_end = train_end + int(total * val_ratio)\n",
    "    \n",
    "        train_images = images[:train_end]\n",
    "        val_images = images[train_end:val_end]\n",
    "        test_images = images[val_end:]\n",
    "    \n",
    "        train_class_dir = os.path.join(output_dir, 'train', class_folder)\n",
    "        val_class_dir = os.path.join(output_dir, 'val', class_folder)\n",
    "        test_class_dir = os.path.join(output_dir,'test',class_folder)\n",
    "    \n",
    "        os.makedirs(train_class_dir, exist_ok=True)\n",
    "        os.makedirs(val_class_dir, exist_ok=True)\n",
    "        os.makedirs(test_class_dir, exist_ok=True)\n",
    "    \n",
    "        for img in train_images:\n",
    "            shutil.copy2(os.path.join(class_path, img), os.path.join(train_class_dir, img))\n",
    "\n",
    "        for img in val_images:\n",
    "            shutil.copy2(os.path.join(class_path, img), os.path.join(val_class_dir, img))\n",
    "\n",
    "        for img in test_images:\n",
    "            shutil.copy2(os.path.join(class_path, img), os.path.join(test_class_dir, img))\n",
    "\n",
    "        print(f\" {class_folder}: {len(train_images)} train / {len(val_images)} val / {len(test_images)} test\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    source = \"../Datasets/combined-cleaned-dataset\"   \n",
    "    destination = \"../Notebooks/the_final_sortdown\"          \n",
    "    split_dataset(source, destination, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "Found 16410 images belonging to 13 classes.\n",
      "\n",
      "Validation Data:\n",
      "Found 4985 images belonging to 13 classes.\n",
      "\n",
      "Testing Data:\n",
      "Found 5047 images belonging to 13 classes.\n"
     ]
    }
   ],
   "source": [
    "# Data Augmentation\n",
    "train_gen = ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    rotation_range = 20,\n",
    "    width_shift_range = 0.1,\n",
    "    height_shift_range = 0.1,\n",
    "    brightness_range = [0.8, 1.2]\n",
    ")\n",
    "\n",
    "print(\"Training Data:\")\n",
    "train_data = train_gen.flow_from_directory(\n",
    "    \"the_final_sortdown/train\",\n",
    "    class_mode='categorical',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "val_gen = ImageDataGenerator(rescale=1./255)\n",
    "test_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "print(\"\\nValidation Data:\")\n",
    "val_data = val_gen.flow_from_directory(\n",
    "    \"the_final_sortdown/val\",\n",
    "    class_mode='categorical',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")\n",
    "print(\"\\nTesting Data:\")\n",
    "test_data = test_gen.flow_from_directory(\n",
    "    \"the_final_sortdown/test\",\n",
    "    class_mode='categorical',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")\n",
    "class_names = list(test_data.class_indices.keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer Learning MobileNetV2\n",
    "base_model = MobileNetV2(\n",
    "    include_top = False,\n",
    "    weights ='imagenet',\n",
    "    input_shape = (224,224,3)\n",
    ")\n",
    "base_model.trainable = False\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "\n",
    "num_classes = len(train_data.class_indices)\n",
    "output = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(),\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',   # What to monitor (validation loss is typical)\n",
    "    patience=5,           # How many epochs without improvement before stopping\n",
    "    restore_best_weights=True  # Optional, but restores the best model, not the last one\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "\u001b[1m513/513\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 236ms/step - accuracy: 0.7597 - loss: 0.7722 - val_accuracy: 0.9089 - val_loss: 0.2838\n",
      "Epoch 2/7\n",
      "\u001b[1m513/513\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 231ms/step - accuracy: 0.8888 - loss: 0.3388 - val_accuracy: 0.9117 - val_loss: 0.2685\n",
      "Epoch 3/7\n",
      "\u001b[1m513/513\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 231ms/step - accuracy: 0.9038 - loss: 0.2728 - val_accuracy: 0.9276 - val_loss: 0.2040\n",
      "Epoch 4/7\n",
      "\u001b[1m513/513\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 236ms/step - accuracy: 0.9198 - loss: 0.2192 - val_accuracy: 0.9292 - val_loss: 0.1998\n",
      "Epoch 5/7\n",
      "\u001b[1m513/513\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 231ms/step - accuracy: 0.9255 - loss: 0.1936 - val_accuracy: 0.9380 - val_loss: 0.1672\n",
      "Epoch 6/7\n",
      "\u001b[1m513/513\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 237ms/step - accuracy: 0.9315 - loss: 0.1787 - val_accuracy: 0.9388 - val_loss: 0.1669\n",
      "Epoch 7/7\n",
      "\u001b[1m513/513\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 232ms/step - accuracy: 0.9384 - loss: 0.1594 - val_accuracy: 0.9509 - val_loss: 0.1332\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_data,\n",
    "    validation_data=val_data,\n",
    "    epochs = 7, \n",
    "    callbacks = [early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 146ms/step - accuracy: 0.9633 - loss: 0.0919\n",
      "Test accuarcy: 0.94\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_data)\n",
    "print(f\"Test accuarcy: {test_acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 147ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     battery       0.93      0.98      0.96       265\n",
      "  biological       0.99      0.99      0.99       268\n",
      " brown-glass       0.84      0.88      0.86       173\n",
      "   cardboard       0.97      0.96      0.97       360\n",
      "     clothes       0.99      0.99      0.99      1493\n",
      "       glass       0.47      0.54      0.50       140\n",
      " green-glass       0.91      0.87      0.89       175\n",
      "       metal       0.92      0.95      0.94       333\n",
      "       paper       0.96      0.98      0.97       470\n",
      "     plastic       0.95      0.94      0.95       381\n",
      "       shoes       0.97      0.99      0.98       545\n",
      "       trash       0.96      0.91      0.94       235\n",
      " white-glass       0.85      0.67      0.75       209\n",
      "\n",
      "    accuracy                           0.94      5047\n",
      "   macro avg       0.90      0.90      0.90      5047\n",
      "weighted avg       0.94      0.94      0.94      5047\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adding Precision, Recall, and F1\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(test_data)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Get true labels\n",
    "y_true = test_data.classes\n",
    "\n",
    "# Print report\n",
    "print(classification_report(y_true, y_pred_classes, target_names=class_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Reinforcement Learning Sysytem\n",
    "\n",
    "import gym \n",
    "from gym import spaces \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WasteDisposalEnv(gym.Env): \n",
    "\n",
    "    def __init__(self, classifier_model, test_data, co2_impact_mapping, class_names, precomputed_probs=None):\n",
    "\n",
    "        super(WasteDisposalEnv, self).__init__()\n",
    "\n",
    "        #used for predicting material type \n",
    "\n",
    "        self.classifier = classifier_model \n",
    "\n",
    "        #data for predict \n",
    "\n",
    "        self.data = test_data\n",
    "        self.class_names = class_names\n",
    "        self.images, self.labels = self._prepare_data()  \n",
    "        self.co2_impact_mapping = co2_impact_mapping \n",
    "        \n",
    "        if precomputed_probs is not None:\n",
    "            self.predictions = precomputed_probs\n",
    "            self.images, self.labels = self._prepare_data_labels_only()  # only get labels\n",
    "\n",
    "        else:\n",
    "            self.images, self.labels = self._prepare_data()  \n",
    "            self.predictions = self.classifier.predict(self.images, verbose=1)\n",
    "\n",
    "    \n",
    "        #define action space 4 disposal options(Recycle, Compost, Donate, Landfill)\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.predictions = precomputed_probs\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(self.classifier.output_shape[-1],), dtype=np.float32)\n",
    "        \n",
    "        self.current_idx = 0\n",
    "\n",
    "\n",
    "    def _prepare_data(self):\n",
    "        images = []\n",
    "        labels = []\n",
    "        for batch_x, batch_y in self.data:\n",
    "            images.append(batch_x())\n",
    "            labels.append(np.argmax(batch_y, axis=1))\n",
    "        return np.concatenate(images), np.concatenate(labels)\n",
    "\n",
    "        \n",
    "    def reset(self): \n",
    "        self.current_idx = 0 \n",
    "        # image = self.images[self.current_idx]\n",
    "        # probs = self.classifier.predict(image[np.newaxis, ...])[0]\n",
    "        return self.predictions[self.current_idx]\n",
    "    \n",
    "    def step(self, action):\n",
    "        image = self.images[self.current_idx]\n",
    "        true_label = self.labels[self.current_idx]\n",
    "\n",
    "        probs = self.predictions[self.current_idx]\n",
    "        predicted_class = np.argmax(probs)\n",
    "\n",
    "        reward = self._calculate_reward(predicted_class, action)\n",
    "\n",
    "        self.current_idx += 1\n",
    "        done = self.current_idx >= len(self.images)\n",
    "\n",
    "        if not done:\n",
    "            next_image = self.images[self.current_idx]\n",
    "            next_probs = self.classifier.predict(next_image[np.newaxis, ...])[0]\n",
    "        else:\n",
    "            next_probs = np.zeros_like(probs)\n",
    "\n",
    "        return next_probs, reward, done, {}\n",
    "        \n",
    "    def _calculate_reward(self, predicted_class, action):\n",
    "        item_type = self.class_names[predicted_class]\n",
    "        impact_info = self.co2_impact_mapping.get(item_type, None)\n",
    "\n",
    "        if impact_info is None:\n",
    "            return -1  # Unknown item, small penalty\n",
    "\n",
    "        max_co2_saved = impact_info[\"max_co2_saved\"]\n",
    "        actions = impact_info[\"actions\"]\n",
    "\n",
    "        action_name = {0: \"recycle\", 1: \"compost\", 2: \"donate\", 3: \"landfill\"}.get(action, None)\n",
    "\n",
    "        if action_name is None:\n",
    "            return -1  # Invalid action penalty\n",
    "\n",
    "        # Get multiplier for action\n",
    "        action_multiplier = actions.get(action_name, 0.0)  # 0 if action not listed\n",
    "\n",
    "        # Reward is proportional\n",
    "        reward = max_co2_saved * action_multiplier\n",
    "        return reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "co2_impact_mapping = {\n",
    "    \"plastic\": {\n",
    "        \"max_co2_saved\": 10,  # in kg CO2\n",
    "        \"actions\": {\n",
    "            \"recycle\": 1.0,    # 100% of CO₂ savings if recycled\n",
    "            \"landfill\": 0.0    # 0% savings if landfilled\n",
    "        }\n",
    "    },\n",
    "    \"food_waste\": {\n",
    "        \"max_co2_saved\": 8,\n",
    "        \"actions\": {\n",
    "            \"compost\": 1.0,\n",
    "            \"landfill\": 0.1    # 10% saving (maybe still some methane capture)\n",
    "        }\n",
    "    },\n",
    "    \"electronics\": {\n",
    "        \"max_co2_saved\": 15,\n",
    "        \"actions\": {\n",
    "            \"donate\": 1.0,\n",
    "            \"landfill\": 0.0\n",
    "        }\n",
    "    },\n",
    "    \"textile\": {\n",
    "        \"max_co2_saved\": 12,\n",
    "        \"actions\": {\n",
    "            \"donate\": 1.0,\n",
    "            \"landfill\": 0.2    # Landfilling might still recover some energy\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 162ms/step\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.ndarray' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m class_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(test_data\u001b[38;5;241m.\u001b[39mclass_indices\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m      2\u001b[0m all_probs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(test_data, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mWasteDisposalEnv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclassifier_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mco2_impact_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mco2_impact_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mclass_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mprecomputed_probs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_probs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m episode_rewards \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     11\u001b[0m co2_savings \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[94], line 15\u001b[0m, in \u001b[0;36mWasteDisposalEnv.__init__\u001b[0;34m(self, classifier_model, test_data, co2_impact_mapping, class_names, precomputed_probs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m test_data\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_names \u001b[38;5;241m=\u001b[39m class_names\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mco2_impact_mapping \u001b[38;5;241m=\u001b[39m co2_impact_mapping \n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m precomputed_probs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[94], line 39\u001b[0m, in \u001b[0;36mWasteDisposalEnv._prepare_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     37\u001b[0m labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_x, batch_y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata:\n\u001b[0;32m---> 39\u001b[0m     images\u001b[38;5;241m.\u001b[39mappend(\u001b[43mbatch_x\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     40\u001b[0m     labels\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39margmax(batch_y, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mconcatenate(images), np\u001b[38;5;241m.\u001b[39mconcatenate(labels)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.ndarray' object is not callable"
     ]
    }
   ],
   "source": [
    "class_names = list(test_data.class_indices.keys())\n",
    "all_probs = model.predict(test_data, verbose=1)\n",
    "\n",
    "env = WasteDisposalEnv(classifier_model=model, \n",
    "                       test_data=test_data, \n",
    "                       co2_impact_mapping=co2_impact_mapping, \n",
    "                       class_names=class_names,\n",
    "                       precomputed_probs=all_probs)\n",
    "\n",
    "episode_rewards = []\n",
    "co2_savings = []\n",
    "\n",
    "num_episodes = 10  # you can change this to 100 later if you want\n",
    "all_probs = model.predict(test_data, verbose=1)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    total_co2_saved = 0\n",
    "     \n",
    "    while not done:\n",
    "        # Random action for now (you can replace with a policy later)\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        next_observation, reward, done, info = env.step(action)\n",
    "\n",
    "        total_reward += reward\n",
    "        total_co2_saved += reward  # because reward is proportional to CO₂ saved\n",
    "\n",
    "        observation = next_observation\n",
    "\n",
    "    episode_rewards.append(total_reward)\n",
    "    co2_savings.append(total_co2_saved)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'episode' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Total Reward = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_reward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Total CO₂ Saved = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_co2_saved\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m kg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'episode' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"Episode {episode}: Total Reward = {total_reward}, Total CO₂ Saved = {total_co2_saved:.2f} kg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot total reward (CO₂ saved) per episode\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(episode_rewards, marker='o')\n",
    "plt.title('Total CO₂ Saved per Episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total CO₂ Saved (kg)')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
